{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/Local-LLM-Comparison-Colab-UI/blob/main/VibeThinker_1_5B_GGUFF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## VibeThinker-1.5B-GGUFF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm to proceed by typing \"Y\")\n",
        "2. Click the gradio link at the bottom\n",
        "3. Chat Template: Qwen3-Thinking\n",
        "4. Recommended params:\n",
        "```\n",
        "temperature = 0.6 or 1.0\n",
        "top_p = 0.95\n",
        "top_k = -1\n",
        "max_token_length = 40960\n",
        "```\n",
        "\n",
        "Quantized model: https://huggingface.co/MaziyarPanahi/VibeThinker-1.5B-GGUF\n",
        "\n",
        "Original model: https://huggingface.co/WeiboAI/VibeThinker-1.5B\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-MiHp_bveP6",
        "outputId": "634be162-1730-4e4b-b002-a3d2164facd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'text-generation-webui' already exists and is not an empty directory.\n",
            "/content/text-generation-webui\n",
            "Ignoring audioop-lts: markers 'python_version >= \"3.13\"' don't match your environment\n",
            "Ignoring triton-windows: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Collecting gradio-client==1.0.2+custom.1 (from -r requirements/full/requirements.txt (line 35))\n",
            "  Downloading https://github.com/oobabooga/gradio/releases/download/custom-build/gradio_client-1.0.2+custom.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring llama-cpp-binaries: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Collecting llama-cpp-binaries==0.61.0+cu124 (from -r requirements/full/requirements.txt (line 44))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.61.0/llama_cpp_binaries-0.61.0+cu124-py3-none-linux_x86_64.whl (1350.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 GB\u001b[0m \u001b[31m622.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring exllamav3: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav3: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Requirement already satisfied: accelerate==1.8.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: bitsandbytes==0.48.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 3)) (0.48.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 5)) (4.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 6)) (0.8.1)\n",
            "Requirement already satisfied: fastapi==0.112.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 7)) (0.112.4)\n",
            "Requirement already satisfied: flash-linear-attention==0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: html2text==2025.4.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 9)) (2025.4.15)\n",
            "Requirement already satisfied: huggingface-hub==0.36.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 10)) (0.36.0)\n",
            "Requirement already satisfied: jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 12)) (3.10)\n",
            "Requirement already satisfied: numpy==2.2.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 13)) (2.2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 14)) (2.2.2)\n",
            "Requirement already satisfied: peft==0.18.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 15)) (0.18.0)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 16)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 17)) (5.9.5)\n",
            "Requirement already satisfied: pydantic==2.11.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 18)) (2.11.0)\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 19)) (3.0.1)\n",
            "Requirement already satisfied: python-docx==1.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 20)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 21)) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 22)) (2.32.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 23)) (13.9.4)\n",
            "Requirement already satisfied: safetensors==0.6.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 24)) (0.6.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 25)) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 26)) (0.2.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 27)) (2.19.0)\n",
            "Requirement already satisfied: transformers==4.57.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 28)) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 30)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 31)) (0.23.0)\n",
            "Requirement already satisfied: gradio==4.37.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 34)) (4.37.2)\n",
            "Requirement already satisfied: flask_cloudflared==0.0.14 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 38)) (0.0.14)\n",
            "Requirement already satisfied: sse-starlette==1.6.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 39)) (1.6.5)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 40)) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 7)) (0.38.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: fla-core==0.4.0 in /usr/local/lib/python3.12/dist-packages (from flash-linear-attention==0.4.0->-r requirements/full/requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2==3.1.6->-r requirements/full/requirements.txt (line 11)) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18)) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18)) (0.4.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx==1.1.2->-r requirements/full/requirements.txt (line 20)) (5.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.*->-r requirements/full/requirements.txt (line 28)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.*->-r requirements/full/requirements.txt (line 28)) (0.22.1)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.0.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (6.5.2)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (3.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.14.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.20.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.38.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.0.2+custom.1->-r requirements/full/requirements.txt (line 35)) (11.0.3)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.12/dist-packages (from flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 38)) (3.1.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 23)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 23)) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.76.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 31)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 31)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 31)) (4.5.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 31)) (2.45.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (2.12.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 38)) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 38)) (2.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 31)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements/full/requirements.txt (line 23)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (3.2.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.22.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 31)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 34)) (0.29.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.3.0)\n",
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "364b2c|\u001b[1;32mOK\u001b[0m  |   113MiB/s|/content/text-generation-webui/user_data/models/VibeThinker-1.5B.fp16.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "/content/text-generation-webui\n",
            "\u001b[2;36m11:01:53-640209\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text Generation Web UI                        \n",
            "\u001b[2;36m11:01:53-898001\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"VibeThinker-1.5B.fp16.gguf\"\u001b[0m                   \n",
            "\u001b[2;36m11:01:53-907720\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using \u001b[33mgpu_layers\u001b[0m=\u001b[1;36m999\u001b[0m | \u001b[33mctx_size\u001b[0m=\u001b[1;36m32768\u001b[0m | \u001b[33mcache_type\u001b[0m=\u001b[35mfp16\u001b[0m\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)\n",
            "\u001b[0mbuild: 1 (10e9780) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "init: using 6 threads for HTTP server\n",
            "Web UI is disabled\n",
            "start: binding port with default address family\n",
            "main: loading model\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 31 key-value pairs and 339 tensors from user_data/models/VibeThinker-1.5B.fp16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Models WeiboAI VibeThinker 1.5B\n",
            "llama_model_loader: - kv   3:                           general.basename str              = models-WeiboAI-VibeThinker\n",
            "llama_model_loader: - kv   4:                         general.size_label str              = 1.5B\n",
            "llama_model_loader: - kv   5:                            general.license str              = mit\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = Qwen2.5 Math 1.5B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Qwen\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen2.5-M...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,4]       = [\"math\", \"code\", \"gpqa\", \"text-genera...\n",
            "llama_model_loader: - kv  11:                          general.languages arr[str,1]       = [\"en\"]\n",
            "llama_model_loader: - kv  12:                          qwen2.block_count u32              = 28\n",
            "llama_model_loader: - kv  13:                       qwen2.context_length u32              = 131072\n",
            "llama_model_loader: - kv  14:                     qwen2.embedding_length u32              = 1536\n",
            "llama_model_loader: - kv  15:                  qwen2.feed_forward_length u32              = 8960\n",
            "llama_model_loader: - kv  16:                 qwen2.attention.head_count u32              = 12\n",
            "llama_model_loader: - kv  17:              qwen2.attention.head_count_kv u32              = 2\n",
            "llama_model_loader: - kv  18:                       qwen2.rope.freq_base f32              = 640000.000000\n",
            "llama_model_loader: - kv  19:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  20:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = qwen2\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 151643\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 151643\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
            "llama_model_loader: - type  f32:  141 tensors\n",
            "llama_model_loader: - type  f16:  198 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = F16\n",
            "print_info: file size   = 3.31 GiB (16.00 BPW) \n",
            "load: printing all EOG tokens:\n",
            "load:   - 151643 ('<|endoftext|>')\n",
            "load:   - 151645 ('<|im_end|>')\n",
            "load:   - 151662 ('<|fim_pad|>')\n",
            "load:   - 151663 ('<|repo_name|>')\n",
            "load:   - 151664 ('<|file_sep|>')\n",
            "load: special tokens cache size = 24\n",
            "load: token to piece cache size = 0.9311 MB\n",
            "print_info: arch             = qwen2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 131072\n",
            "print_info: n_embd           = 1536\n",
            "print_info: n_embd_inp       = 1536\n",
            "print_info: n_layer          = 28\n",
            "print_info: n_head           = 12\n",
            "print_info: n_head_kv        = 2\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 6\n",
            "print_info: n_embd_k_gqa     = 256\n",
            "print_info: n_embd_v_gqa     = 256\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-06\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 8960\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: n_expert_groups  = 0\n",
            "print_info: n_group_used     = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = -1\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 640000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 131072\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 1.5B\n",
            "print_info: model params     = 1.78 B\n",
            "print_info: general.name     = Models WeiboAI VibeThinker 1.5B\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 151936\n",
            "print_info: n_merges         = 151387\n",
            "print_info: BOS token        = 11 ','\n",
            "print_info: EOS token        = 151643 '<|endoftext|>'\n",
            "print_info: EOT token        = 151645 '<|im_end|>'\n",
            "print_info: PAD token        = 151643 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: FIM PRE token    = 151659 '<|fim_prefix|>'\n",
            "print_info: FIM SUF token    = 151661 '<|fim_suffix|>'\n",
            "print_info: FIM MID token    = 151660 '<|fim_middle|>'\n",
            "print_info: FIM PAD token    = 151662 '<|fim_pad|>'\n",
            "print_info: FIM REP token    = 151663 '<|repo_name|>'\n",
            "print_info: FIM SEP token    = 151664 '<|file_sep|>'\n",
            "print_info: EOG token        = 151643 '<|endoftext|>'\n",
            "print_info: EOG token        = 151645 '<|im_end|>'\n",
            "print_info: EOG token        = 151662 '<|fim_pad|>'\n",
            "print_info: EOG token        = 151663 '<|repo_name|>'\n",
            "print_info: EOG token        = 151664 '<|file_sep|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 28 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 29/29 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =   445.12 MiB\n",
            "load_tensors:        CUDA0 model buffer size =  2944.68 MiB\n",
            ".........\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 4\n",
            "llama_context: n_ctx         = 32768\n",
            "llama_context: n_ctx_seq     = 32768\n",
            "llama_context: n_batch       = 256\n",
            "llama_context: n_ubatch      = 256\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = enabled\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 640000.0\n",
            "llama_context: freq_scale    = 1\n",
            "llama_context: n_ctx_seq (32768) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "\u001b[0mllama_context:  CUDA_Host  output buffer size =     2.32 MiB\n",
            "llama_kv_cache:      CUDA0 KV buffer size =   896.00 MiB\n",
            "llama_kv_cache: size =  896.00 MiB ( 32768 cells,  28 layers,  4/1 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   177.62 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =    33.51 MiB\n",
            "llama_context: graph nodes  = 959\n",
            "llama_context: graph splits = 2\n",
            "common_init_from_params: added <|endoftext|> logit bias = -inf\n",
            "common_init_from_params: added <|im_end|> logit bias = -inf\n",
            "common_init_from_params: added <|fim_pad|> logit bias = -inf\n",
            "common_init_from_params: added <|repo_name|> logit bias = -inf\n",
            "common_init_from_params: added <|file_sep|> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 32768\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "\u001b[0msrv          init: initializing slots, n_slots = 4\n",
            "\u001b[0msrv          init: use `--cache-ram 0` to disable the prompt cache\n",
            "\u001b[0msrv          init: for more info see https://github.com/ggml-org/llama.cpp/pull/16391\n",
            "\u001b[0msrv          init: thinking = 0\n",
            "init: chat template, chat_template: {%- if tools %}\n",
            "    {{- '<|im_start|>system\\n' }}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- messages[0]['content'] }}\n",
            "    {%- else %}\n",
            "        {{- '' }}\n",
            "    {%- endif %}\n",
            "    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
            "    {%- for tool in tools %}\n",
            "        {{- \"\\n\" }}\n",
            "        {{- tool | tojson }}\n",
            "    {%- endfor %}\n",
            "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
            "{%- else %}\n",
            "    {%- if messages[0]['role'] == 'system' %}\n",
            "        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n",
            "    {%- else %}\n",
            "        {{- '<|im_start|>system\\nPlease reason step by step, and put your final answer within \\\\boxed{}.<|im_end|>\\n' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- for message in messages %}\n",
            "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n",
            "        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n",
            "    {%- elif message.role == \"assistant\" %}\n",
            "        {{- '<|im_start|>' + message.role }}\n",
            "        {%- if message.content %}\n",
            "            {{- '\\n' + message.content }}\n",
            "        {%- endif %}\n",
            "        {%- for tool_call in message.tool_calls %}\n",
            "            {%- if tool_call.function is defined %}\n",
            "                {%- set tool_call = tool_call.function %}\n",
            "            {%- endif %}\n",
            "            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n",
            "            {{- tool_call.name }}\n",
            "            {{- '\", \"arguments\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- '}\\n</tool_call>' }}\n",
            "        {%- endfor %}\n",
            "        {{- '<|im_end|>\\n' }}\n",
            "    {%- elif message.role == \"tool\" %}\n",
            "        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n",
            "            {{- '<|im_start|>user' }}\n",
            "        {%- endif %}\n",
            "        {{- '\\n<tool_response>\\n' }}\n",
            "        {{- message.content }}\n",
            "        {{- '\\n</tool_response>' }}\n",
            "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
            "            {{- '<|im_end|>\\n' }}\n",
            "        {%- endif %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|im_start|>assistant\\n' }}\n",
            "{%- endif %}\n",
            ", example_format: '<|im_start|>system\n",
            "You are a helpful assistant<|im_end|>\n",
            "<|im_start|>user\n",
            "Hello<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Hi there<|im_end|>\n",
            "<|im_start|>user\n",
            "How are you?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "'\n",
            "main: model loaded\n",
            "main: server is listening on http://127.0.0.1:39779\n",
            "main: starting the main loop...\n",
            "\u001b[2;36m11:01:56-930627\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"VibeThinker-1.5B.fp16.gguf\"\u001b[0m in \u001b[1;36m3.03\u001b[0m seconds.   \n",
            "\u001b[2;36m11:01:56-932001\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                    \n",
            "\u001b[2;36m11:01:56-932839\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m32768\u001b[0m                               \n",
            "\u001b[2;36m11:01:56-933575\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model \u001b[0m    \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mmetadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m                                             \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://57aebce7846988de4a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "prompt processing progress, n_tokens = 43, batch.n_tokens = 43, progress = 1.000000\n",
            "prompt eval time =     272.61 ms /    43 tokens (    6.34 ms per token,   157.73 tokens per second)\n",
            "       eval time =  234318.61 ms / 12769 tokens (   18.35 ms per token,    54.49 tokens per second)\n",
            "      total time =  234591.22 ms / 12812 tokens\n",
            "\u001b[2;36m11:13:37-906709\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output generated in \u001b[1;36m234.63\u001b[0m seconds \u001b[1m(\u001b[0m\u001b[1;36m54.42\u001b[0m tokens/s,    \n",
            "\u001b[2;36m                \u001b[0m         \u001b[1;36m12768\u001b[0m tokens, context \u001b[1;36m43\u001b[0m, seed \u001b[1;36m1800870413\u001b[0m\u001b[1m)\u001b[0m             \n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "!git clone -b V20251124 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements/full/requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/MaziyarPanahi/VibeThinker-1.5B-GGUF/resolve/main/VibeThinker-1.5B.fp16.gguf?download=true -d /content/text-generation-webui/user_data/models -o VibeThinker-1.5B.fp16.gguf\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --n-gpu-layers 999 --model VibeThinker-1.5B.fp16.gguf --n_ctx 32768"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdINsX8a6C5xueY1pjdFnL",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}