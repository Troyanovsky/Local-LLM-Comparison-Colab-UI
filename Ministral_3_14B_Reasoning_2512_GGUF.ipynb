{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Troyanovsky/Local-LLM-Comparison-Colab-UI/blob/main/Ministral_3_14B_Reasoning_2512_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkpBtLvuTOp"
      },
      "source": [
        "## Ministral-3-14B-Reasoning-2512-GGUF WebUI\n",
        "\n",
        "1. Run the following cell, takes ~5 min\n",
        "(You may need to confirm to proceed by typing \"Y\")\n",
        "2. Click the gradio link at the bottom\n",
        "3. Chat Template: Custom (fixed by Unsloth)\n",
        "\n",
        "Quantized model: https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF\n",
        "\n",
        "Original model: https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512\n",
        "\n",
        "Want to try other local LLMs? Check out this repo: https://github.com/Troyanovsky/Local-LLM-Comparison-Colab-UI/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D-MiHp_bveP6",
        "outputId": "8521963e-d519-4b4c-b831-b078aa6f8505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Cloning into 'text-generation-webui'...\n",
            "remote: Enumerating objects: 26049, done.\u001b[K\n",
            "remote: Counting objects: 100% (190/190), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 26049 (delta 174), reused 137 (delta 136), pack-reused 25859 (from 4)\u001b[K\n",
            "Receiving objects: 100% (26049/26049), 30.39 MiB | 39.34 MiB/s, done.\n",
            "Resolving deltas: 100% (18945/18945), done.\n",
            "Note: switching to '4d94f668327129c92a497fd776649f712e394447'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "/content/text-generation-webui\n",
            "Ignoring audioop-lts: markers 'python_version >= \"3.13\"' don't match your environment\n",
            "Ignoring triton-windows: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Collecting gradio-client==1.0.2+custom.1 (from -r requirements/full/requirements.txt (line 36))\n",
            "  Downloading https://github.com/oobabooga/gradio/releases/download/custom-build/gradio_client-1.0.2+custom.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.3/318.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers@ git+https://github.com/huggingface/diffusers.git@edf36f5128abf3e6ecf92b5145115514363c58e6 (from -r requirements/full/requirements.txt (line 39))\n",
            "  Cloning https://github.com/huggingface/diffusers.git (to revision edf36f5128abf3e6ecf92b5145115514363c58e6) to /tmp/pip-install-vzmpz_ac/diffusers_354836983b734143b435d4d05e744929\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-install-vzmpz_ac/diffusers_354836983b734143b435d4d05e744929\n",
            "  Running command git rev-parse -q --verify 'sha^edf36f5128abf3e6ecf92b5145115514363c58e6'\n",
            "  Running command git fetch -q https://github.com/huggingface/diffusers.git edf36f5128abf3e6ecf92b5145115514363c58e6\n",
            "  Running command git checkout -q edf36f5128abf3e6ecf92b5145115514363c58e6\n",
            "  Resolved https://github.com/huggingface/diffusers.git to commit edf36f5128abf3e6ecf92b5145115514363c58e6\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Ignoring llama-cpp-binaries: markers 'platform_system == \"Windows\"' don't match your environment\n",
            "Collecting llama-cpp-binaries==0.66.0+cu124 (from -r requirements/full/requirements.txt (line 48))\n",
            "  Downloading https://github.com/oobabooga/llama-cpp-binaries/releases/download/v0.66.0/llama_cpp_binaries-0.66.0+cu124-py3-none-linux_x86_64.whl (1431.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 GB\u001b[0m \u001b[31m675.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hIgnoring exllamav3: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav3: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring exllamav2: markers 'platform_system == \"Linux\" and platform_machine != \"x86_64\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Windows\" and python_version == \"3.11\"' don't match your environment\n",
            "Ignoring flash-attn: markers 'platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version == \"3.11\"' don't match your environment\n",
            "Collecting accelerate==1.8.* (from -r requirements/full/requirements.txt (line 1))\n",
            "  Downloading accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting bitsandbytes==0.48.* (from -r requirements/full/requirements.txt (line 3))\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting colorama (from -r requirements/full/requirements.txt (line 4))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 5)) (4.0.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 6)) (0.8.1)\n",
            "Collecting fastapi==0.112.4 (from -r requirements/full/requirements.txt (line 7))\n",
            "  Downloading fastapi-0.112.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting flash-linear-attention==0.4.0 (from -r requirements/full/requirements.txt (line 8))\n",
            "  Downloading flash_linear_attention-0.4.0-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting html2text==2025.4.15 (from -r requirements/full/requirements.txt (line 9))\n",
            "  Downloading html2text-2025.4.15-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: huggingface-hub==0.36.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 10)) (0.36.0)\n",
            "Requirement already satisfied: jinja2==3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 11)) (3.1.6)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 12)) (3.10)\n",
            "Collecting numpy==2.2.* (from -r requirements/full/requirements.txt (line 13))\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 14)) (2.2.2)\n",
            "Requirement already satisfied: peft==0.18.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 15)) (0.18.0)\n",
            "Requirement already satisfied: Pillow>=9.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 16)) (11.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 17)) (5.9.5)\n",
            "Collecting pydantic==2.11.0 (from -r requirements/full/requirements.txt (line 18))\n",
            "  Downloading pydantic-2.11.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2==3.0.1 (from -r requirements/full/requirements.txt (line 19))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx==1.1.2 (from -r requirements/full/requirements.txt (line 20))\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 21)) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 22)) (2.32.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 23)) (13.9.4)\n",
            "Collecting safetensors==0.6.* (from -r requirements/full/requirements.txt (line 24))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 25)) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 26)) (0.2.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 27)) (2.19.0)\n",
            "Collecting torchao==0.14.* (from -r requirements/full/requirements.txt (line 28))\n",
            "  Downloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers==4.57.* in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 29)) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 31)) (4.67.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 32)) (0.23.0)\n",
            "Collecting gradio==4.37.* (from -r requirements/full/requirements.txt (line 35))\n",
            "  Downloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting flask_cloudflared==0.0.14 (from -r requirements/full/requirements.txt (line 42))\n",
            "  Downloading flask_cloudflared-0.0.14-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting sse-starlette==1.6.5 (from -r requirements/full/requirements.txt (line 43))\n",
            "  Downloading sse_starlette-1.6.5-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from -r requirements/full/requirements.txt (line 44)) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.9.0+cu126)\n",
            "Collecting starlette<0.39.0,>=0.37.2 (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 7))\n",
            "  Downloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi==0.112.4->-r requirements/full/requirements.txt (line 7)) (4.15.0)\n",
            "Collecting fla-core==0.4.0 (from flash-linear-attention==0.4.0->-r requirements/full/requirements.txt (line 8))\n",
            "  Downloading fla_core-0.4.0-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub==0.36.0->-r requirements/full/requirements.txt (line 10)) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2==3.1.6->-r requirements/full/requirements.txt (line 11)) (3.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18)) (0.7.0)\n",
            "Collecting pydantic-core==2.33.0 (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18))\n",
            "  Downloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.11.0->-r requirements/full/requirements.txt (line 18)) (0.4.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx==1.1.2->-r requirements/full/requirements.txt (line 20)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.*->-r requirements/full/requirements.txt (line 29)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.*->-r requirements/full/requirements.txt (line 29)) (0.22.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (5.5.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.0.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (6.5.2)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2==3.1.6->-r requirements/full/requirements.txt (line 11))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (3.10.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (3.11.4)\n",
            "Collecting Pillow>=9.5.0 (from -r requirements/full/requirements.txt (line 16))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.14.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.20.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.38.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2+custom.1->-r requirements/full/requirements.txt (line 36))\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.12/dist-packages (from flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 42)) (3.1.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->-r requirements/full/requirements.txt (line 5)) (0.70.16)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements/full/requirements.txt (line 14)) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements/full/requirements.txt (line 22)) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 23)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->-r requirements/full/requirements.txt (line 23)) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.76.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements/full/requirements.txt (line 27)) (3.1.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 32)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 32)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 32)) (4.5.0)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements/full/requirements.txt (line 32)) (2.46.0)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers@ git+https://github.com/huggingface/diffusers.git@edf36f5128abf3e6ecf92b5145115514363c58e6->-r requirements/full/requirements.txt (line 39)) (8.7.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (2.12.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 42)) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask>=0.8->flask_cloudflared==0.0.14->-r requirements/full/requirements.txt (line 42)) (2.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 32)) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements/full/requirements.txt (line 23)) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (3.2.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (3.5.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.5.4)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers@ git+https://github.com/huggingface/diffusers.git@edf36f5128abf3e6ecf92b5145115514363c58e6->-r requirements/full/requirements.txt (line 39)) (3.23.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->-r requirements/full/requirements.txt (line 5)) (1.22.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.24.1->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (1.3.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements/full/requirements.txt (line 32)) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.37.*->-r requirements/full/requirements.txt (line 35)) (0.29.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.8.*->-r requirements/full/requirements.txt (line 1)) (1.3.0)\n",
            "Downloading accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.112.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flash_linear_attention-0.4.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading html2text-2025.4.15-py3-none-any.whl (34 kB)\n",
            "Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.0-py3-none-any.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchao-0.14.1-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m125.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cloudflared-0.0.14-py3-none-any.whl (6.4 kB)\n",
            "Downloading sse_starlette-1.6.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading fla_core-0.4.0-py3-none-any.whl (438 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.33.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading starlette-0.38.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: diffusers\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.36.0.dev0-py3-none-any.whl size=4562773 sha256=df122d491756c52c954511d0efced84efa39057efd6c97ac724f2c9835754726\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/e8/cc/09dfab5d35c43e2e47002c0f5585fa6d1d972164a51774dac0\n",
            "Successfully built diffusers\n",
            "Installing collected packages: torchao, websockets, tomlkit, safetensors, python-docx, PyPDF2, pydantic-core, Pillow, numpy, MarkupSafe, llama-cpp-binaries, html2text, colorama, aiofiles, starlette, pydantic, sse-starlette, gradio-client, fastapi, diffusers, flask_cloudflared, fla-core, bitsandbytes, accelerate, gradio, flash-linear-attention\n",
            "  Attempting uninstall: torchao\n",
            "    Found existing installation: torchao 0.10.0\n",
            "    Uninstalling torchao-0.10.0:\n",
            "      Successfully uninstalled torchao-0.10.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.7.0\n",
            "    Uninstalling safetensors-0.7.0:\n",
            "      Successfully uninstalled safetensors-0.7.0\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.4\n",
            "    Uninstalling pydantic_core-2.41.4:\n",
            "      Successfully uninstalled pydantic_core-2.41.4\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.48.0\n",
            "    Uninstalling starlette-0.48.0:\n",
            "      Successfully uninstalled starlette-0.48.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.3\n",
            "    Uninstalling pydantic-2.12.3:\n",
            "      Successfully uninstalled pydantic-2.12.3\n",
            "  Attempting uninstall: sse-starlette\n",
            "    Found existing installation: sse-starlette 3.0.3\n",
            "    Uninstalling sse-starlette-3.0.3:\n",
            "      Successfully uninstalled sse-starlette-3.0.3\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.118.3\n",
            "    Uninstalling fastapi-0.118.3:\n",
            "      Successfully uninstalled fastapi-0.118.3\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.35.2\n",
            "    Uninstalling diffusers-0.35.2:\n",
            "      Successfully uninstalled diffusers-0.35.2\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "google-genai 1.52.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "google-adk 1.19.0 requires fastapi<0.119.0,>=0.115.0, but you have fastapi 0.112.4 which is incompatible.\n",
            "google-adk 1.19.0 requires starlette<1.0.0,>=0.46.2, but you have starlette 0.38.6 which is incompatible.\n",
            "google-adk 1.19.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 Pillow-10.4.0 PyPDF2-3.0.1 accelerate-1.8.1 aiofiles-23.2.1 bitsandbytes-0.48.2 colorama-0.4.6 diffusers-0.36.0.dev0 fastapi-0.112.4 fla-core-0.4.0 flash-linear-attention-0.4.0 flask_cloudflared-0.0.14 gradio-4.37.2 gradio-client-1.0.2+custom.1 html2text-2025.4.15 llama-cpp-binaries-0.66.0+cu124 numpy-2.2.6 pydantic-2.11.0 pydantic-core-2.33.0 python-docx-1.1.2 safetensors-0.6.2 sse-starlette-1.6.5 starlette-0.38.6 tomlkit-0.12.0 torchao-0.14.1 websockets-11.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "18a1b7f5a35845d2b1d1cb79abdd0733"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "5ef0e3|\u001b[1;32mOK\u001b[0m  |   175MiB/s|/content/text-generation-webui/user_data/models/Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "/content/text-generation-webui\n",
            "\u001b[2;36m07:50:29-002390\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text Generation Web UI                        \n",
            "\u001b[2;36m07:50:29-290868\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf\"\u001b[0m   \n",
            "\u001b[2;36m07:50:29-298902\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using \u001b[33mgpu_layers\u001b[0m=\u001b[1;36m999\u001b[0m | \u001b[33mctx_size\u001b[0m=\u001b[1;36m32768\u001b[0m | \u001b[33mcache_type\u001b[0m=\u001b[35mfp16\u001b[0m\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: setting n_parallel = 4 and kv_unified = true (add -kvu to disable this)\n",
            "\u001b[0mbuild: 1 (0a540f9) with GNU 11.4.0 for Linux x86_64\n",
            "system info: n_threads = 1, n_threads_batch = 1, total_threads = 2\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
            "init: using 6 threads for HTTP server\n",
            "Web UI is disabled\n",
            "start: binding port with default address family\n",
            "main: loading model\n",
            "llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 48 key-value pairs and 363 tensors from user_data/models/Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = mistral3\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Ministral-3-14B-Reasoning-2512\n",
            "llama_model_loader: - kv   3:                            general.version str              = 2512\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Reasoning\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Ministral-3-14B-Reasoning-2512\n",
            "llama_model_loader: - kv   6:                       general.quantized_by str              = Unsloth\n",
            "llama_model_loader: - kv   7:                         general.size_label str              = 14B\n",
            "llama_model_loader: - kv   8:                           general.repo_url str              = https://huggingface.co/unsloth\n",
            "llama_model_loader: - kv   9:                       mistral3.block_count u32              = 40\n",
            "llama_model_loader: - kv  10:                    mistral3.context_length u32              = 262144\n",
            "llama_model_loader: - kv  11:                  mistral3.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv  12:               mistral3.feed_forward_length u32              = 16384\n",
            "llama_model_loader: - kv  13:              mistral3.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  14:           mistral3.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  15:  mistral3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  16:              mistral3.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  17:            mistral3.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  18:                        mistral3.vocab_size u32              = 131072\n",
            "llama_model_loader: - kv  19:              mistral3.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                 mistral3.rope.scaling.type str              = yarn\n",
            "llama_model_loader: - kv  21:               mistral3.rope.scaling.factor f32              = 16.000000\n",
            "llama_model_loader: - kv  22:       mistral3.rope.scaling.yarn_beta_fast f32              = 32.000000\n",
            "llama_model_loader: - kv  23:       mistral3.rope.scaling.yarn_beta_slow f32              = 1.000000\n",
            "llama_model_loader: - kv  24:  mistral3.rope.scaling.yarn_log_multiplier f32              = 1.000000\n",
            "llama_model_loader: - kv  25: mistral3.rope.scaling.original_context_length u32              = 16384\n",
            "llama_model_loader: - kv  26:                    mistral3.rope.freq_base f32              = 1000000000.000000\n",
            "llama_model_loader: - kv  27:       mistral3.attention.temperature_scale f32              = 0.100000\n",
            "llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = tekken\n",
            "llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,131072]  = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
            "llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,131072]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
            "llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,269443]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ �...\n",
            "llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  35:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  36:            tokenizer.ggml.padding_token_id u32              = 11\n",
            "llama_model_loader: - kv  37:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  38:               tokenizer.ggml.add_sep_token bool             = false\n",
            "llama_model_loader: - kv  39:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  40:                    tokenizer.chat_template str              = {#- Unsloth template fixes #}\\n{#- Def...\n",
            "llama_model_loader: - kv  41:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  42:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  43:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  44:                      quantize.imatrix.file str              = Ministral-3-14B-Reasoning-2512-GGUF/i...\n",
            "llama_model_loader: - kv  45:                   quantize.imatrix.dataset str              = unsloth_calibration_Ministral-3-14B-R...\n",
            "llama_model_loader: - kv  46:             quantize.imatrix.entries_count u32              = 280\n",
            "llama_model_loader: - kv  47:              quantize.imatrix.chunks_count u32              = 139\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type q4_K:  241 tensors\n",
            "llama_model_loader: - type q6_K:   41 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 7.67 GiB (4.88 BPW) \n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "\u001b[0mload: printing all EOG tokens:\n",
            "load:   - 2 ('</s>')\n",
            "load: special tokens cache size = 1000\n",
            "load: token to piece cache size = 0.8498 MB\n",
            "print_info: arch             = mistral3\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 262144\n",
            "print_info: n_embd           = 5120\n",
            "print_info: n_embd_inp       = 5120\n",
            "print_info: n_layer          = 40\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: is_swa_any       = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 16384\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: n_expert_groups  = 0\n",
            "print_info: n_group_used     = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = yarn\n",
            "print_info: freq_base_train  = 1000000000.0\n",
            "print_info: freq_scale_train = 0.0625\n",
            "print_info: n_ctx_orig_yarn  = 16384\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: model type       = 14B\n",
            "print_info: model params     = 13.51 B\n",
            "print_info: general.name     = Ministral-3-14B-Reasoning-2512\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 131072\n",
            "print_info: n_merges         = 269443\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: PAD token        = 11 '<pad>'\n",
            "print_info: LF token         = 1010 'Ċ'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 150\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: offloading 40 repeating layers to GPU\n",
            "load_tensors: offloading output layer to GPU\n",
            "load_tensors: offloaded 41/41 layers to GPU\n",
            "load_tensors:   CPU_Mapped model buffer size =   360.00 MiB\n",
            "load_tensors:        CUDA0 model buffer size =  7489.86 MiB\n",
            "....................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_seq_max     = 4\n",
            "llama_context: n_ctx         = 32768\n",
            "llama_context: n_ctx_seq     = 32768\n",
            "llama_context: n_batch       = 1024\n",
            "llama_context: n_ubatch      = 1024\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = enabled\n",
            "llama_context: kv_unified    = true\n",
            "llama_context: freq_base     = 1000000000.0\n",
            "llama_context: freq_scale    = 0.0625\n",
            "llama_context: n_ctx_seq (32768) < n_ctx_train (262144) -- the full capacity of the model will not be utilized\n",
            "llama_context:  CUDA_Host  output buffer size =     2.00 MiB\n",
            "llama_kv_cache:      CUDA0 KV buffer size =  5120.00 MiB\n",
            "llama_kv_cache: size = 5120.00 MiB ( 32768 cells,  40 layers,  4/1 seqs), K (f16): 2560.00 MiB, V (f16): 2560.00 MiB\n",
            "llama_context:      CUDA0 compute buffer size =   532.00 MiB\n",
            "llama_context:  CUDA_Host compute buffer size =   148.03 MiB\n",
            "llama_context: graph nodes  = 1287\n",
            "llama_context: graph splits = 2\n",
            "common_init_from_params: added </s> logit bias = -inf\n",
            "common_init_from_params: setting dry_penalty_last_n to ctx_size = 32768\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "\u001b[0msrv          init: initializing slots, n_slots = 4\n",
            "\u001b[0msrv          init: use `--cache-ram 0` to disable the prompt cache\n",
            "\u001b[0msrv          init: for more info see https://github.com/ggml-org/llama.cpp/pull/16391\n",
            "\u001b[0msrv          init: thinking = 0\n",
            "init: chat template, chat_template: {#- Unsloth template fixes #}\n",
            "{#- Default system message if no system prompt is passed. #}\n",
            "{%- set default_system_message = '# HOW YOU SHOULD THINK AND ANSWER\\n\\nFirst draft your thinking process (inner monologue) until you arrive at a response. Format your response using Markdown, and use LaTeX for any mathematical equations. Write both your thoughts and the response in the same language as the input.\\n\\nYour thinking process must follow the template below:[THINK]Your thoughts or/and draft, like working through an exercise on scratch paper. Be as casual and as long as you want until you are confident to generate the response to the user.[/THINK]Here, provide a self-contained response.' %}\n",
            "{#- Begin of sequence token. #}\n",
            "{{- bos_token }}\n",
            "{#- Handle system prompt if it exists. #}\n",
            "{#- System prompt supports text content or text and thinking chunks. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {{- '[SYSTEM_PROMPT]' -}}\n",
            "    {%- if messages[0]['content'] is string %}\n",
            "        {{- messages[0]['content'] -}}\n",
            "    {%- else %}        \n",
            "        {%- for block in messages[0]['content'] %}\n",
            "            {%- if block['type'] == 'text' %}\n",
            "                {{- block['text'] }}\n",
            "            {%- elif block['type'] == 'thinking' %}\n",
            "                {{- '[THINK]' + block['thinking'] + '[/THINK]' }}\n",
            "            {%- else %}\n",
            "                {{- raise_exception('Only text and thinking chunks are supported in system message contents.') }}\n",
            "            {%- endif %}\n",
            "        {%- endfor %}\n",
            "    {%- endif %}\n",
            "    {{- '[/SYSTEM_PROMPT]' -}}\n",
            "    {%- set loop_messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set loop_messages = messages %}\n",
            "    {%- if default_system_message != '' %}\n",
            "        {{- '[SYSTEM_PROMPT]' + default_system_message + '[/SYSTEM_PROMPT]' }}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{#- Tools definition #}\n",
            "{%- set tools_definition = '' %}\n",
            "{%- set has_tools = false %}\n",
            "{%- if tools is defined and tools is not none and tools|length > 0 %}\n",
            "    {%- set has_tools = true %}\n",
            "    {%- set tools_definition = '[AVAILABLE_TOOLS]' + (tools| tojson) + '[/AVAILABLE_TOOLS]' %}\n",
            "    {{- tools_definition }}\n",
            "{%- endif %}\n",
            "{#- Checks for alternating user/assistant messages. #}\n",
            "{%- set ns = namespace(index=0) %}\n",
            "{%- for message in loop_messages %}\n",
            "    {%- if message.role == 'user' or (message.role == 'assistant' and (message.tool_calls is not defined or message.tool_calls is none or message.tool_calls | length == 0)) %}\n",
            "        {%- if (message['role'] == 'user') != (ns.index % 2 == 0) %}\n",
            "            {{- raise_exception('After the optional system message, conversation roles must alternate user and assistant roles except for tool calls and results.') }}\n",
            "        {%- endif %}\n",
            "        {%- set ns.index = ns.index + 1 %}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{#- Handle conversation messages. #}\n",
            "{%- for message in loop_messages %}\n",
            "    {#- User messages supports text content or text and image chunks. #}\n",
            "    {%- if message['role'] == 'user' %}\n",
            "        {%- if message['content'] is string %}\n",
            "            {{- '[INST]' + message['content'] + '[/INST]' }}\n",
            "        {%- elif message['content'] | length > 0 %}\n",
            "            {{- '[INST]' }}\n",
            "            {%- if message['content'] | length == 2 %}\n",
            "                {%- set blocks = message['content'] | sort(attribute='type') %}\n",
            "            {%- else %}\n",
            "                {%- set blocks = message['content'] %}\n",
            "            {%- endif %}\n",
            "            {%- for block in blocks %}\n",
            "                {%- if block['type'] == 'text' %}\n",
            "                    {{- block['text'] }}\n",
            "                {%- elif block['type'] in ['image', 'image_url'] %}\n",
            "                    {{- '[IMG]' }}\n",
            "                {%- else %}\n",
            "                    {{- raise_exception('Only text, image and image_url chunks are supported in user message content.') }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "            {{- '[/INST]' }}\n",
            "        {%- else %}\n",
            "            {{- raise_exception('User message must have a string or a list of chunks in content') }}\n",
            "        {%- endif %}\n",
            "    {#- Assistant messages supports text content or text, image and thinking chunks. #}\n",
            "    {%- elif message['role'] == 'assistant' %}\n",
            "        {%- if message['content'] is string and message['content'] != '' %}\n",
            "            {{- message['content'] }}\n",
            "        {%- elif message['content'] is iterable and message['content'] | length > 0 %}\n",
            "            {%- for block in message['content'] %}\n",
            "                {%- if block['type'] == 'text' %}\n",
            "                    {{- block['text'] }}\n",
            "                {%- elif block['type'] == 'thinking' %}\n",
            "                    {{- '[THINK]' + block['thinking'] + '[/THINK]' }}\n",
            "                {%- else %}\n",
            "                    {{- raise_exception('Only text and thinking chunks are supported in assistant message contents.') }}\n",
            "                {%- endif %}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        \n",
            "        {%- if message['tool_calls'] is defined and message['tool_calls'] is not none and message['tool_calls']|length > 0 %}\n",
            "            {%- for tool in message['tool_calls'] %}\n",
            "                {{- '[TOOL_CALLS]' }}\n",
            "                {%- set name = tool['function']['name'] %}\n",
            "                {%- set arguments = tool['function']['arguments'] %}\n",
            "                {%- if arguments is not string %}\n",
            "                    {%- set arguments = arguments|tojson|safe %}\n",
            "                {%- elif arguments == '' %}\n",
            "                    {%- set arguments = '{}' %}\n",
            "                {%- endif %}\n",
            "                {{- name + '[ARGS]' + arguments }}\n",
            "            {%- endfor %}\n",
            "        {%- endif %}\n",
            "        {{- eos_token }}\n",
            "    {#- Tool messages only supports text content. #}\n",
            "    {%- elif message['role'] == 'tool' %}\n",
            "        {{- '[TOOL_RESULTS]' + message['content']|string + '[/TOOL_RESULTS]' }}\n",
            "    {#- Raise exception for unsupported roles. #}\n",
            "    {%- else %}\n",
            "        {{- raise_exception('Only user, assistant and tool roles are supported, got ' + message['role'] + '.') }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{#- Copyright 2025-present Unsloth. Apache 2.0 License. #}, example_format: '[SYSTEM_PROMPT]You are a helpful assistant[/SYSTEM_PROMPT][INST]Hello[/INST]Hi there</s>[INST]How are you?[/INST]'\n",
            "main: model loaded\n",
            "main: server is listening on http://127.0.0.1:54041\n",
            "main: starting the main loop...\n",
            "\u001b[2;36m07:51:01-382974\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[32m\"Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf\"\u001b[0m in \n",
            "\u001b[2;36m                \u001b[0m         \u001b[1;36m32.09\u001b[0m seconds.                                         \n",
            "\u001b[2;36m07:51:01-384483\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"llama.cpp\"\u001b[0m                                    \n",
            "\u001b[2;36m07:51:01-385511\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m32768\u001b[0m                               \n",
            "\u001b[2;36m07:51:01-386525\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model \u001b[0m    \n",
            "\u001b[2;36m                \u001b[0m         \u001b[32mmetadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m                                             \n",
            "\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "Running on public URL: https://c28148b89270f74d62.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "prompt processing progress, n_tokens = 143, batch.n_tokens = 143, progress = 1.000000\n",
            "prompt eval time =     506.38 ms /   143 tokens (    3.54 ms per token,   282.39 tokens per second)\n",
            "       eval time =   46954.64 ms /   870 tokens (   53.97 ms per token,    18.53 tokens per second)\n",
            "      total time =   47461.02 ms /  1013 tokens\n",
            "\u001b[2;36m07:55:23-249653\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Output generated in \u001b[1;36m47.47\u001b[0m seconds \u001b[1m(\u001b[0m\u001b[1;36m18.29\u001b[0m tokens/s, \u001b[1;36m868\u001b[0m \n",
            "\u001b[2;36m                \u001b[0m         tokens, context \u001b[1;36m143\u001b[0m, seed \u001b[1;36m1642744373\u001b[0m\u001b[1m)\u001b[0m                  \n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!apt-get -y install -qq aria2\n",
        "\n",
        "!git clone -b V20251208 https://github.com/Troyanovsky/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements/full/requirements.txt\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF/resolve/main/Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf?download=true -d /content/text-generation-webui/user_data/models -o Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --share --n-gpu-layers 999 --model Ministral-3-14B-Reasoning-2512-Q4_K_M.gguf --n_ctx 32768"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7Z3ad31HjMAOK2jEEy4At",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}